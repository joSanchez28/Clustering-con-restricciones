\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[spanish, es-tabla]{babel}
\usepackage{caption}
\usepackage{listings}
\usepackage{adjustbox}
%\usepackage{enumitem}
\usepackage{boldline}
\usepackage{amssymb, amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
%\usepackage{soul}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{graphics, graphicx, float}
\usepackage{minted}

% Meta
\title{Problema de Agrupamiento con Restricciones (PAR)
	\\\medskip \large Búsquedas por trayectorias para el PAR \\\medskip
	\large Metaheurísticas: Práctica 3.b, Grupo MH2 (Jueves de 17:30h a 19:30h)}
\author{Jorge Sánchez González - 75569829V \\ jorgesg97@correo.ugr.es}
\date{ \today }

% Custom
\providecommand{\abs}[1]{\lvert#1\rvert}
\setlength\parindent{0pt}
\definecolor{Light}{gray}{.90}
\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}
\setlength{\parindent}{1.5em} %sangria

% Displaying code with lstlisting
\lstset { %
	language=C++,
	backgroundcolor=\color{black!5}, % set backgroundcolor
	basicstyle=\footnotesize,% basic font setting
}

\usepackage[ruled]{algorithm2e}


\begin{document}	
	
	\maketitle 
	\newpage
	\tableofcontents
	\newpage
	
	
	\section{El problema}
	
	\subsection{Descripción del problema}\label{sec:problema}
	El \textbf{agrupamiento} o \textbf{análisis de clusters} clásico (en inglés, \emph{clustering}) es un problema que persigue la clasificación de objetos de acuerdo a posibles similitudes entre ellos. Así, se trata de una técnica de aprendizaje no supervisado que permite clasificar en grupos (desconocidos a priori) objetos de un conjunto de datos que tienen características similares.
	
    El \textbf{Problema del Agrupamiento con Restricciones (PAR)} (en
    inglés, Constrained Clustering, CC) es una
    generalización del problema del agrupamiento clásico.
    Permite incorporar al proceso de agrupamiento un nuevo tipo
    de información: las restricciones. Al incorporar esta información la tarea de aprendizaje deja de ser no supervisada y se convierte en semi-supervisada. 
    
    El problema consiste en, dado un conjunto de datos $X$ con $n$ instancias, encontrar una partición $C$ del mismo que minimice la desviación general y cumpla con las restricciones del conjunto de restricciones $R$. En nuestro caso concreto, solo consideramos restricciones de instancia (Must-Link o Cannot-Link) y todas ellas las interpretaremos como restricciones débiles (Soft); con lo que la partición $C$ del conjunto de datos $X$ debe minimizar el número de restricciones incumplidas pero puede incumplir algunas. Formalmente, buscamos
    $$ \text{Minimizar } f = \hat{C} + \lambda * infeasibility$$
	
	donde:
	\begin{itemize}
		\item $C$ es una partición (una solución al problema), que consiste en una asignación de cada instancia a un cluster.
		\item $\hat{C}$ es la desviación general de la partición $C$, que se define como la media de las desviaciones intra-cluster.
		\item $infeasibility$ es el número de restricciones que $C$ incumple.
		\item $\lambda$ es un hyperparámetro a ajustar (en función de si le queremos dar más importancia a las restricciones o a la desviación general).
		
	\end{itemize}

	\subsection{Conjuntos de datos considerados}
	Trabajaremos con 6 instancias del PAR generadas a partir de los 3 conjuntos de datos siguientes:
	\begin{enumerate}
        \item Iris: Información sobre las características de tres tipos de flor de Iris. Se trata de 150 instancias con 4 características por cada una de ellas. Tiene 3 clases ($k = 3$).
        \item Ecoli: Contiene medidas sobre ciertas características de diferentes tipos de células. Se trata de 336 instancias con 7 características. Tiene 8 clases ($k = 8$).
        \item Rand: Está formado por tres agrupamientos bien diferenciados generados en base a distribuciones normales ($k = 3$). Se trata de 150 instancias con 2 características.
        \item Newthyroid: Contiene medidas tomadas sobre la glándula tiroides de múltiples pacientes. Presenta 3 clases distintas ($k = 3$) y se trata de 215 instancias con 5 características.
  \end{enumerate}
    
    Para cada conjunto de datos se trabaja con 2 conjuntos de restricciones, correspondientes al 10\% y 20\% del total de restricciones posibles.
	
	
	\section{Descripción de la aplicación de los algoritmos}\label{sec:comun}
	
	En esta sección se describen las consideraciones comunes a los distintos algoritmos. Se incluyen la representación de las soluciones, la función objetivo y los operadores comunes a los distintos algoritmos. Ya que los únicos puntos en común de todos los algoritmos (incluyendo los de la práctica 1 y la 3) son la función objetivo y la representación de las soluciones, no estudiaremos ningún operador común. No se han incluido tampoco los detalles específicos de ninguno de los algoritmos en esta sección.  \\
	
	Una primera consideración general es el hecho de que la distancia usada como medida de cercanía (o similitud) entre las distintas instancias y clusters es la distancia euclídea. \\
	
	El lenguaje utilizado para la implementación de la práctica ha sido \textbf{Python}. 
	
	\subsection{Representación de la soluciones}
	Las soluciones a nuestro problema serán las llamadas \textit{particiones}, que asignan a cada instancia del conjunto de datos uno de los $k$ clusters. Aunque existen varias formas de representar una partición, por simplicidad se ha decidido utilizar la misma forma en todos los algoritmos. En concreto, respresentamos una partición de un conjunto de $n$ instancias en $k$ clusters con una lista $S$ de tamaño $n$ cuyos valores son enteros $S_i \in \{0,1,2,...,k-1\}$. Así, el valor de la posición $i$ del vector, $S_i$, indica el cluster al que la i-ésima instancia, $x_i$, ha sido asignada. 
	
	Por verlo con un ejemplo, la representación será de la siguiente forma:

	\begin{lstlisting}
	partition = [0,0,0,5,0,1,1,1,1,1,2,3,1,1,1,...]
	\end{lstlisting}
	Esta partición indicaría, por ejemplo, que la instancia $x_0$ se ha asignado al cluster número 0, $c_0$, y que la instancia $x_3$ se ha asignado al cluster $c_5$. 
	
	Cabe mencionar también que durante la ejecución de los algoritmos, las particiones en proceso de construcción tendrán sus valores en $\{-1,0,1,2,...,k-1\}$. Cuando una posición $i$ tenga el valor $-1$, esto indicará que aún no se le ha asignado ningún cluster a la instancia $x_i$.
	
	\subsection{Función objetivo}
	Como hemos visto en la seccion anterior (\ref{sec:problema}) para calcular la función objetivo se necesita saber tanto la desviación general de la partición, $\hat{C}$, como el número de restricciones que esta viola, $infeasibility$. Se muestra a continuación su expresión y su pseudocódigo. 
	
	$$f(C) = \hat{C} + \lambda * infeasibility$$

	\begin{algorithm}
	 	\caption{objective\_function}
	 	\KwData{Conjunto de datos $X$, partición en forma de vector $S$, lista de restricciones $constraints\_list$, lista de centroides $centroids$, hiperparámetro $lambda$}
	 	\KwResult{function\_value}
	 	\Begin{
	 	    dev $\leftarrow$ general\_deviation(X, S, centroids) \\
	 	    inf $\leftarrow$ $infeasibility$(S, constraints\_list) \\
            function\_value $\leftarrow$ dev + lambda * inf\\
	 		return function\_value \\
	 	}
	\end{algorithm}
	
	El hyperparámetro $\lambda$ que se usará por defecto será el mínimo indicado en las diapositivas del Seminario 2, es decir, el cociente entre la distancia máxima existente en el conjunto de datos y el número de restricciones del problema. 
	
	Cabe mencionar que la función objetivo como tal será usada por todos los algoritmos excepto por el Greedy, que usará la desviación general y la infactibilidad en diferentes etapas (y por separado). De cualquier manera, la calidad de las soluciones de todos los algoritmos va a ser evaluada con esta función.
	\subsubsection{Desviación general de una partición}
	La desviación general de una partición se define como la media de las distancias medias intra-cluster, esto es,
	$$\hat{C} = \frac{1}{k} \sum_{c_i \in C} \hat{c_i}.$$ Donde las distancias medias intra-cluster se definen a su vez como $$\hat{c_i} = \frac{1}{|c_i|} \sum_{\vec{x_j} \in c_i}distance(\vec{x_j}, \vec{\mu_i}).$$ En pseudocódigo:
	
    \begin{algorithm} 
	 	\caption{mean\_dist\_intra\_cluster} \label{algo:mean-dist-intra-cluster}
	 	\KwData{Conjunto de datos $X$, partición $S$, cluster del que queremos calcular su distancia intra cluster $cluster\_id$, centroide del cluster $centroid$}
	 	\KwResult{distance\_intra\_cluster}
	 	\Begin{
	 	    $Y$ $\leftarrow$ instancias\_asignadas\_al\_cluster(X, S, cluster\_id)\\
	 	    \If{centroid == None}{
	 	            centroid = mean(Y, axis = 0)\\
	 	    }
	 	    distances $\leftarrow$ list([ ]) \\
	 	    \ForEach{ $y \in Y$ }{
	 	        distances.append(distance(y, centroid))
	 	    }
	 	    distance\_intra\_cluster $\leftarrow$ media(distances) \\
	 		return distance\_intra\_cluster \\
	 	}
	\end{algorithm}
	\begin{algorithm}
	 	\caption{general\_deviation}
	 	\KwData{Conjunto de datos $X$, partición $S$, lista de centroides $centroids$}
	 	\KwResult{deviation}
	 	\Begin{
	 	    cluster\_ids $\leftarrow$ \{0,1,...,longitud(centroids)-1\} \\
	 	    intra\_cluster\_distances $\leftarrow$ list([ ]) \\
	 	    \ForEach{$c_{id} \in cluster\_ids$}{
	 	        d $\leftarrow$ mean\_dist\_intra\_cluster(X, S, $c_{id}$, centroids[$c_{id}$])\\
	 	        intra\_cluster\_distances.append(d)\\
	 	    }
	 	    deviation $\leftarrow$ media(intra\_cluster\_distances) \\
	 		return deviation \\
	 	}
	\end{algorithm}
	Cabe comentar que la lista de centroides que se pasa como argumento es opcional, y en caso de no pasarse se calculará cada centroide directamente a la hora de calcular su distancia media intra-cluster asociada (como indica el condicional \textit{if} en la función mean\_dist\_intra\_cluster (\ref{algo:mean-dist-intra-cluster})).
	
    \subsubsection{Infactibilidad}
    La infactibilidad de una partición ($infeasibility$) se define como el número de restricciones que incumple. Por ello para calcularla utilizamos una función auxiliar, \textit{V}, que nos va a decir, dada una partición, y un par de instancias con un valor de restricción (-1,0 o 1), si la partición incumple alguna restricción asociada a las dos instancias. Por otro lado, cabe destacar también que usamos la lista de restricciones, y no la matriz de restricciones, para recorrer todas las restricciones para calcular el $infeasibility$ de una manera más eficiente.
    \begin{algorithm}
	 	\caption{V (si se incumple alguna restricción o no)}
	 	\KwData{Índice $i$ de la instancia $x_i$, índice $j$ de la instancia $x_j$, partición $S$, valor de restricción $constraint\_value$)}
	 	\KwResult{incumplimiento}
	 	\Begin{
	 	    incumplimiento\_ML $\leftarrow$ ($constraint\_value == 1$ and $S\left[i\right] \neq  S\left[j\right]$)\\
	 	    incumplimiento\_CL $\leftarrow$ ($constraint\_value == -1$ and $S\left[i\right] ==  S\left[j\right]$)\\
	 	    incumplimiento $\leftarrow$ (incumplimiento\_ML or incumplimiento\_CL) \\
	 		return incumplimiento \\
	 	}
	\end{algorithm}
	\begin{algorithm}
	 	\caption{$infeasibility$}
	 	\KwData{Partición $S$, lista de restricciones $constraints\_list$}
	 	\KwResult{infeasibility}
	 	\Begin{
	 	    infeasibility $\leftarrow$ 0\\
	 	    \ForEach{ $(i,j,contraint\_value) \in $ constraints\_list}{
	 	        infeasibility $\leftarrow$ infeasibility + V(i, j, S, constraint\_value) \\
	 	    }
	 		return infeasibility \\
	 	}
	\end{algorithm}
	\section{Algoritmos}
	En esta práctica se han usado seis algoritmos. El Greedy (\textit{COPKM}) y el de Búsqueda Local (ya implementados en la práctica 1) ya se habían ejecutado sobre todos los conjuntos de datos en las prácticas anteriores. Los otros 4 son el de enfriamiento simulado (\textit{ES}), búsqueda local multiarranque (\textit{BMB}), búsqueda local reiterada (\textit{ILS}) y la hibridación de \textit{ILS} y \textit{ES} (\textit{ILS-ES}). Estos se han implementado y se han ejecutado sobre todos los conjuntos de datos. La descripción de los dos primeros algoritmos  ya se vió en la primera práctica. No obstante, dada la relevancia de la búsqueda local en el resto de algoritmos (es una componente de algunos) y el hecho de que se han realizado pequeñas modificaciones en su implementación daremos una descripción del algoritmo en esta memoria también. El resto de algoritmos (los de la práctica 3) también se describen a continuación mostrando pseudocódigo y comentando los aspectos más importantes. 
	
	\subsection{Generación de soluciones iniciales aleatorias}
	Un primer punto en común de todos los algoritmos que vamos a describir (esto es, de todos los algoritmos que compararemos excepto el Greedy) es su necesidad de generar soluciones aleatorias en cierta etapa del algoritmo. En concreto, el algoritmo \textit{BMB} tiene la necesidad de generar una solución inicial aleatoria cada vez que finaliza una búsqueda local y reinicia para hacer otra. El resto de algoritmos solo tienen esta necesidad en su inicio, en el que generan una solución inicial aleatoria para partir de ella. Describimos a continuación en pseudocódigo el proceso de generación de soluciones aleatorias.
	
	 El proceso se divide en dos funciones. La primera de ellas, \textit{generate\_initial\_sol(X, k)}, genera una partición de forma aleatoria sin tener en cuenta la restricción fuerte de que no haya ningún cluster con 0 instancias asignadas.
	
    \begin{algorithm}[H]
	 	\caption{generate\_initial\_sol}
	 	\KwData{Conjunto de datos $X$, número de clusters $k$}
	 	\KwResult{Una partición $S$}
	 	\Begin{
	 	    n $\leftarrow$ longitud(X);
	 	    S $\leftarrow$ list([ ])\\
	 	    \ForEach{ $i \in \{0,1,...,n-1\}$}{
	 	        S.append(entero\_aleatorio\_entre(0,k-1))\\
	 	    }
	 		return S \\
	 	}
	\end{algorithm}
	
	La segunda, \textit{generate\_valid\_initial\_sol(X, k)}, hace uso de esta primera función y se asegura de que se devuelve una solución que cumpla que asigna a cada uno de los $k$ clusters al menos una instancia, o lo que es lo mismo, una solución "válida".
	
	\begin{algorithm}[H]
	 	\caption{generate\_valid\_initial\_sol}
	 	\KwData{Conjunto de datos $X$, número de clusters $k$}
	 	\KwResult{Una partición válida $S$ y su contador de asignaciones $assignations\_counter$}
	 	\Begin{
	 	    valid\_partition $\leftarrow$ False \\
            \While{not valid\_partition}{
                S $\leftarrow$ generate\_initial\_sol(X, k) \\
                assignations\_counter $\leftarrow$ cuenta\_asignaciones(S) \\
                \If{asignaciones\_validas(assignations\_counter)}{
                    valid\_partition $\leftarrow$ True \\
                }
            }
            return S, assignations\_counter \\
	 	}
	\end{algorithm}
	Cabe comentar que no solo se devuelve la solución generada, sino también un diccionario ($assignations\_counter$) con la cuenta de asignaciones de cada cluster que se podrá usar para comprobar que no se rompe la restricción fuerte (de no asignar ninguna instancia a algún cluster) cada vez que se modifique la partición.
	
	\subsection{Búsqueda local}
	
	Tratamos ahora el algoritmo búsqueda local paso por paso. Primero se generará una partición de forma aleatoria como acabamos de describir en la sección anterior.
	
	Una vez tengamos la partición inicial, en cada iteración exploramos el vecindario hasta encontrar una solución mejor y sustituimos la actual por la encontrada. Repetiremos este proceso hasta que exploremos todo el vecindario y no encontremos una solución mejor o hasta que hayamos evaluado la función objetivo $max\_evaluations$ veces.
	
	Con el objetivo de poder reutilizar las funciones de este algoritmo en algoritmos posteriores (en concreto en el \textit{ILS}), se ha dividido el algoritmo en dos funciones. Una primera ($local\_search\_from\_partition$) en la que se lanza una búsqueda local con un punto de partida (una solución inicial $S$) dado como argumento. Se describe en pseudocódigo a continuación.
	
	\begin{algorithm}
	 	\caption{local\_search\_from\_partition}
	 	\KwData{Solución inicial S, contador de asignaciones de S $assignations\_counter$, conjunto de datos $X$, lista de restricciones $const\_list$, número de clusters $k$, hiperparámetro $\lambda$, número máximo de evaluaciones de la función objetivo $max\_evaluations$}
	 	\KwResult{Partición solución S}
	 	\Begin{
	 	    n $\leftarrow$ longitud(X)\\
            current\_func\_value $\leftarrow$ objective\_function(X, S, const\_list, $\lambda$) \\
            counter $\leftarrow$ 1 (Número de veces que se evalua la función objetivo) \\
            -Creamos una lista de parejas [(0,+1),(0,+2),..]. Esta lista representará todas las operaciones de movimiento posibles desde una partición dada. \\
            virtual\_neighborhood $\leftarrow$ [(index, add) for index in \{0,1,...,n-1\} for add in \{1,2,...,k\}] \\
            found\_better\_sol $\leftarrow$ True \\
	 	    \While{counter $< max\_evaluations$ and found\_better\_sol}{
	 	        found\_better\_sol $\leftarrow$ False \\
	 	        virtual\_neighborhood $\leftarrow$ RandomShuffle(virtual\_neighborhood) \\
	 	        i $\leftarrow$ 0 \\
	 	        \While{counter $< max\_evaluations$ and not found\_better\_sol and i $<$ n}{
	 	            operation $\leftarrow$ virtual\_neighborhood[i] \\
	 	            -Ejecutamos la operación \\
	 	            tmp $\leftarrow$ S[operation[0]] \\
	 	            S[operation[0]] $\leftarrow$ (S[operation[0]] + operation[1]) mod k \\
                    func\_val $\leftarrow$ objective\_function(X, S, const\_list, $\lambda$) \\
                    counter $\leftarrow$ counter + 1 \\
                    -Si llegamos a una mejor partición que sea válida, la elegimos. \\
                    \eIf{func\_val $<$ current\_func\_value and assignations\_counter[tmp] $>$ 1}{ 
                        assignations\_counter[tmp] $\leftarrow$  assignations\_counter[tmp] - 1 \\
                        assignations\_counter[partition[operation[0]]] $\leftarrow$ assignations\_counter[partition[operation[0]]] + 1 \\
                        current\_func\_value $\leftarrow$ func\_val \\
                        found\_better\_sol $\leftarrow$ True \\
                    }
                    {
                    -Si no, volvemos a la particición anterior \\
                        S[operation[0]] $\leftarrow$ tmp  \\
                    }
                    i $\leftarrow$ i + 1 \\
	 	        }
	 	    }
	 		return S \\
	 	}
	\end{algorithm}
	Y una segunda función ($local\_search$) en la que se tiene como punto de partida una solución generada de forma aleatoria. Esta segunda función hace uso de la primera, y es la que se corresponde con el algoritmo original de la práctica 1 (el que veremos como \textit{BL} más adelante en los resultados). 
	\begin{algorithm}
	 	\caption{local\_search} \label{algo:BL}
	 	\KwData{Conjunto de datos $X$, lista de restricciones $const\_list$, número de clusters $k$, hiperparámetro $\lambda$, número máximo de evaluaciones de la función objetivo $max\_evaluations$}
	 	\KwResult{Partición solución S}
	 	\Begin{
	 	    S, assignations\_counter $\leftarrow$ generate\_valid\_initial\_sol(X, k)\\
	 		return local\_search\_from\_partition(S, $assignations\_counter$, X, $const\_list$, $k$, $\lambda$, $max\_evaluations$)\\
	 	}
	\end{algorithm}
	
	\subsection{Enfriamiento simulado}
	El Enfriamiento Simulado (\textit{ES}) es un algoritmo de búsqueda por entornos con un criterio probabilístico de aceptación de soluciones basado en termodinámica. Este criterio depende del parámetro temperatura en cada etapa del algoritmo, que a su vez quedará determinado por la temperatura inicial y el esquema de enfriamiento que hayamos elegido. En nuestra implementación, siguiendo las instrucciones del guión se ha decidido tomar la temperatura inicial como $$T_0 = \frac{\mu \cdot f(S_0)}{-ln(\phi)},$$
	siendo $f(S_0)$ la función objetivo evaluada en la solución inicial, y $\phi$ la probabilidad de aceptar una solución un $\mu$ por 1 peor que la inicial (en nuestro caso siempre usamos $\mu = \phi = 0.3$). 
	
	Por su parte, el esquema de enfriamiento usado es el de Cauchy modificado, el cuál viene descrito por las siguientes expresiones.
	$$T_{k+1} = \frac{T_k}{1+ \beta \cdot T_k} \;;\;\;\; \beta = \frac{T_0-T_f}{M\cdot T_0 \cdot T_f},$$
	donde M es el número de enfriamientos a realizar y $T_f$ es la temperatura final. En nuestra implementación, siguiendo las indicaciones del guión usaremos $M= max\_evaluations / max\_neighbours$ y $T_f= 10^{-3}$; discutiremos más adelante el valor de $max\_evaluations$ y $max\_neighbours$.
	
	\begin{algorithm}[H]
	 	\caption{cauchy\_scheme}
	 	\KwData{Valor actual del parámetro temperatura $current\_temp$, parámetro beta $\beta$}
	 	\KwResult{Siguiente valor del parámetro temperatura}
	 	\Begin{
	 	    return current\_temp/($1 + \beta*current\_temp$)
	 	}
	\end{algorithm}
	De nuevo con el objetivo de poder reutilizar las funciones de este algoritmo en algoritmos posteriores (en concreto en el \textit{ILS-ES}), se ha dividido el algoritmo en dos funciones. Una primera ($simulated\_annealing\_from\_partition$) en la que se lanza el algortimo con un punto de partida (una solución inicial $S$) dado como argumento. \\ 
	\begin{algorithm}[H]
	 	\caption{simulated\_annealing\_from\_partition} \label{algo:ES}
	 	\KwData{Solución inicial $current\_partition$, asignaciones de esta $assignations\_counter$, conjunto de datos $X$, lista de restricciones $const\_list$, número de clusters $k$, hiperparámetro $\lambda$, máximas evaluaciones de la función objetivo $max\_evaluations$}
	 	\KwResult{Partición solución S}
	 	\Begin{
	 	    n $\leftarrow$ longitud(X)\\
            best\_partition  $\leftarrow$ current\_partition \\
            current\_func\_value $\leftarrow$ objective\_function(X, current\_partition, const\_list, $\lambda$) \\
            best\_func\_value $\leftarrow$ current\_func\_value \\
            %counter $\leftarrow$ 1 (Veces que se evalua la función objetivo) \\
            initial\_temp $\leftarrow$ ($0.3*current\_func\_value$)/$(-\log(0.3))$ \\
            current\_temp $\leftarrow$ initial\_temp \\
            final\_temp $\leftarrow$ $10^{-3}$ \\
            \While{final\_temp $\geq$ initial\_temp}{ 
                final\_temp $\leftarrow$ final\_temp * $10^{-3}$ \\
            }
            max\_neighbours $\leftarrow$ 10 * n * max\_evaluations/100000 -Escalamos max\_neighbours \\
            max\_successes $\leftarrow$ 0.1 * max\_neighbours \\
            m\_iterations $\leftarrow$ max\_evaluations/max\_neighbours \\
            beta $\leftarrow$ (initial\_temp - final\_temp)/(m\_iterations * initial\_temp * final\_temp) \\
            annealings $\leftarrow$ 0;
            successes $\leftarrow$ 1 \\
            \While{annealings$<$m\_iterations and successes $> 0$}{
                successes $\leftarrow$ 0;
                generated\_neighbours $\leftarrow$ 0 \\
                \While{generated\_neighbours $<$ max\_neighbours and successes $<$ max\_successes}{
                    -Generamos un vecino comprobando que no rompe la restriccion fuerte \\
                    gen\_idx $\leftarrow$ entero\_aleatorio\_entre(0,n-1) \\
                    \While{assignations\_counter[current\_partition[gen\_idx]] $\leq 1$}{
                        gen\_idx$\leftarrow$ entero\_aleatorio\_entre(0,n-1) \\
                    }
                    previous\_gen\_value = current\_partition[gen\_idx] \\
                    current\_partition[gen\_idx] $\leftarrow$ (current\_partition[gen\_idx] + entero\_aleatorio\_entre(1,k-1)) mod k \\
                    candidate\_func\_value $\leftarrow$ objective\_func(X, current\_partition, const\_list,$\lambda$) \\
                    %counter += 1; 
                    generated\_neighbours += 1\\
                    increment\_f $\leftarrow$ candidate\_func\_value - current\_func\_value \\
                    \eIf{(increment\_f $< 0$ or aleatorio\_entre(0,1) $\leq$ exp(-increment\_f/current\_temp))}{
                        successes += 1 \\
                        current\_func\_value $\leftarrow$ candidate\_func\_value \\
                        assignations\_counter[previous\_gen\_value] -= 1 \\
                        assignations\_counter[current\_partition[gen\_idx]] += 1 \\
                        \If{current\_func\_value $<$ best\_func\_value}{
                            best\_func\_value $\leftarrow$ current\_func\_value \\
                            best\_partition $\leftarrow$ current\_partition \\
                        }
                    }{
                        -Si  la solución no es aceptada volvemos a la partición anterior \\
                        current\_partition[gen\_idx] $\leftarrow$ previous\_gen\_value  \\
                    }
                }
                annealings += 1; 
                current\_temp $\leftarrow$ cauchy\_scheme(current\_temp, beta) \\
            }
	 		return best\_partition \\
	 	}
	\end{algorithm}
	Y una segunda función ($simulated\_annealing\_algo$) en la que se tiene como punto de partida una solución generada de forma aleatoria. Esta segunda función hace uso de la primera, y es la que se corresponde con el algoritmo original de la práctica 1 (el que veremos como \textit{ES} más adelante en los resultados). 
	\begin{algorithm}
	 	\caption{simulated\_annealing\_algo}
	 	\KwData{Conjunto de datos $X$, lista de restricciones $const\_list$, número de clusters $k$, hiperparámetro $\lambda$, número máximo de evaluaciones de la función objetivo $max\_evaluations$}
	 	\KwResult{Partición solución S}
	 	\Begin{
	 	    S, assignations\_counter $\leftarrow$ generate\_valid\_initial\_sol(X, k)\\
	 		return simulated\_annealing\_from\_partition(S, $assignations\_counter$, X, $const\_list$, $k$, $\lambda$, $max\_evaluations$)\\
	 	}
	\end{algorithm}
	
	Descrito el algoritmo en pseudocódigo, comentamos algunos aspectos importantes. El primer aspecto a comentar es la comprobación de que la temperatura final sea menor que la temperatura inicial (ver primer bucle "while" del algoritmo  \ref{algo:ES}). Esta comprobación se nos sugiere hacerla en el guión de la práctica y, en caso de que tengamos $T_0 \leq T_f = 10^{-3}$ lo que hacemos es multiplicar $T_f$ por $10^{-3}$ para que su valor disminuya hasta que obtener $T_f < T_0$. De cualquier forma, experimentalmente se ha comprobado que en ninguna de nuestras ejecuciones (las que se presentan en la sección de resultados) se da este caso por lo que la comprobación carece de relevancia práctica en nuestra tarea. 
	
	En segundo lugar, dada una iteración del algoritmo los valores de las variables \textit{max\_neighbours} y \textit{max\_successes} nos indican el número máximo de soluciones vecinas a generar y el número máximo de soluciones a aceptar antes de pasar al siguiente enfriamiento. En el guión de la práctica se nos indica que usemos $max\_neighbours = 10*n$ donde $n$ es el número de instancias del conjunto de datos ($X$) y $max\_successes = 0.1 * max\_neighbours$. Estos valores están adaptados para una ejecución individual del algoritmo y considerando $max\_evaluations=100000$. Sin embargo, cuando usamos el algoritmo dentro de \textit{ILS-ES}, donde se lanzan varios enfriamientos simulados con $max\_evaluations=10000$, mantener esos parámetros provoca que se hagan muy pocas iteraciones (pues recordemos que el número de iteraciones es $M= max\_evaluations / max\_neighbours$), dañando la efectividad del algoritmo. Es por ello que se ha decidido escalar el valor \textit{max\_neighbours} con respecto al número de evaluaciones $max\_evaluations$ de la forma que se muestra en el pseudocódigo: $$max\_neighbours = 10*n*(max\_evaluations/100000).$$
	Al hacer el ajuste en $max\_neighbours$, las variables $M$ y $max\_successes$ se ajustan de forma proporcional (pues como hemos visto se calculan a partir de $max\_neighbours$). Así, esta pequeña modificación no supone ningún cambio cuando el algoritmo se ejecuta de forma individual con $max\_evaluations=100000$ y mejora su comportamiento cuando se ejecuta dentro de \textit{ILS-ES}.
	
	\subsection{Búsqueda Multiarranque Básica}
	El algoritmo de Búsqueda Multiarranque Básica \textit{BMB} consiste simplemente en generar un determinado número de soluciones aleatorias iniciales y optimizar cada una de ellas con el algoritmo de búsqueda local. Finalmente se devolverá la mejor solución encontrada en todo el proceso. Se describe a continuación en pseudocódigo (ver algoritmo \ref{algo:BMB}).
	
	\begin{algorithm}
	 	\caption{random\_restart\_local\_search} \label{algo:BMB}
	 	\KwData{Conjunto de datos $X$, lista de restricciones $const\_list$, número de clusters $k$, hiperparámetro $\lambda$, número de búsquedas locales a ejecutar $n\_ls$, número máximo de evaluaciones de la función objetivo en cada búsqueda local $evaluations\_per\_ls$}
	 	\KwResult{Mejor solución encontrada $best\_partition$}
	 	\Begin{
	 	    best\_partition, best\_func\_value $\leftarrow$ local\_search(X, const\_matrix, const\_list, k, $\lambda$, max\_evaluations = evaluations\_per\_ls, restart\_mode = True) \\
            \ForEach{$i \in \{0,1,\dots, n\_ls-2\}$}{
                partition, func\_value $\leftarrow$ local\_search(X, const\_matrix, const\_list, k, $\lambda$, max\_evaluations = evaluations\_per\_ls, restart\_mode = True)\\
                \If{func\_value $<$ best\_func\_value}{
                    best\_func\_value $\leftarrow$ func\_value \\
                    best\_partition $\leftarrow$ partition \\
                }
            }
            return best\_partition \\
	 	}
	\end{algorithm}
	Como vemos, una vez vista la búsqueda local original, este algoritmo es bastante sencillo. Cabe destacar el argumento $restart\_mode = True$ de la búsqueda local, el cual no explicamos en el algoritmo original (algoritmo \ref{algo:BL}) porque carecía de interés para el algoritmo propiamente. Este argumento lo único que hace es indicarle a la función (en este caso a \textit{local\_search}) que no solo devuelva la mejor partición solución encontrada, sino también el valor de la función objetivo asociado a esta partición. Esto nos permite no tener que llamar a la función objetivo tras cada búsqueda local para comparar la calidad de las soluciones dadas por las distintas búsquedas locales. 
	
	En nuestro caso, el número de búsquedas locales que fijamos es $n\_ls = 10$ (por lo que en la práctica la ventaja del argumento $restart\_mode$ no tiene mucha relevancia); y el número máximo de evaluaciones de la función objetivo en cada búsqueda local es $evaluations\_per\_ls=10000$.
	
	\subsection{Búsqueda Local Reiterada (ILS) y algoritmo híbrido ILS-ES}
	Describimos los algortimos ILS e ILS-ES en esta misma sección porque en esencia son el mismo salvo porque uno (el ILS) usará repetidamente la búsqueda local mientras que el otro (ILS-ES) usará repetidamente el algoritmo de enfriamiento simulado. En concreto, el algoritmo consiste en generar una solución inicial aleatoria y aplicar el algoritmo de búsqueda (\textit{BL} o \textit{ES}) sobre ella. Una vez obtenida una solución optimizada, se comprobará si es mejor que la mejor solución encontrada hasta el momento y se realizará una mutación sobre la mejor de estas dos, volviendo a aplicar el algoritmo de búsqueda (\textit{BL} o \textit{ES}) sobre esta solución mutada. Se repite este proceso varias veces y finalmente se devuelve la mejor solución de todas.
	
	Comenzamos describiendo en pseudocódigo el operador de mutación que usa el algoritmo, esto es, el operador de mutación por segmento (ver algoritmo \ref{algo:SMutation}). Vemos que el operador consiste en coger un "segmento de genes" de la solución que empiece en una posición aleatoria y tenga una determinada longitud y cambiar los genes de dicho segmento asignándoles clusters de forma aleatoria. En nuestro caso, usaremos siempre $segment\_length = 0.1* n$ siendo $n$ el número de instancias del conjunto de datos con el que estemos trabajando. Un aspecto de implementación que cabe mencionar es que este operador de mutación no cambia la partición que se le pasa como argumento, sino que crea y devuelve una nueva (esto es importante porque, como veremos más adelante, se le pasará como argumento la mejor solución encontrada hasta el momento).
	
	Además, es claro que este operador podría dar lugar a particiones que rompieran la restricción fuerte de que cada cluster tenga al menos una instancia asignada, por lo que necesitamos una función que las repare en dicho caso, y que se puede describir en pseudocódigo como se observa en el bloque \ref{algo:repair}. Esta función ya fue usada en la práctica anterior.\\
	\begin{algorithm}[ht]
	 	\caption{segment\_mutation\_operator} \label{algo:SMutation}
	 	\KwData{Partición a mutar $S$, longitud del segmento $segment\_length$, número de clusters $k$}
	 	\KwResult{Mutación de la partición dada, $mut\_partition$}
	 	\Begin{
	 	    n $\leftarrow$ longitud(S)\\
	 	    segment\_ini $\leftarrow$ entero\_aleatorio\_entre(0, n-1) \\
	 	    segment\_indices $\leftarrow$ [i mod(n) for $i \in$ \{segment\_ini,..., segment\_ini + segment\_lenght\}] \\
	 	    mut\_partition = copy(S)\\
	 	    \ForEach{$i \in segment\_indices$}{
	 	        mut\_partition[i] $\leftarrow$ entero\_aleatorio\_entre(0, k-1) \\
	 	    }
            return mut\_partition
	 	}
	\end{algorithm} 
	
	\begin{algorithm}[H]
	 	\caption{repair\_partition}\label{algo:repair}
	 	\KwData{Partición a reparar \textit{S}, diccionario con número de instancias asignadas a cada cluster \textit{assignations\_counter}, número de clusters \textit{k}}
	 	\KwResult{Partición reparada \textit{S} y diccionario de asignaciones actualizado \textit{assignations\_counter}}
	 	\Begin{
	 	    n $\leftarrow$ longitud(S)\\
	 	    \ForEach{$i \in \{0,1,...,k-1\}$}{
	 	        \If{assignations\_counter[i] == 0}{
	 	            gen\_idx $\leftarrow$ entero\_aleatorio\_entre(0,n-1) \\ \While{assignations\_counter[S[gen\_idx]] $\leq 1$}{
	 	                gen\_idx $\leftarrow$ entero\_aleatorio\_entre(0,n-1) \\
	 	            }
	 	            assignations\_counter[S[gen\_idx]] -= 1 \\
                    S[gen\_idx] $\leftarrow$ i \\
                    assignations\_counter[i] $\leftarrow$ 1 \\
	 	        }
	 	    }
            return S, assignations\_counter
	 	}
	\end{algorithm} 
	Básicamente la reparación de particiones consiste en asignar una instancia escogida de forma aleatoria a los clusters que no tienen ninguna instancia asignada (siempre comprobando que al hacer esta nueva asignación no estamos dejando a otro cluster con $0$ instancias asignadas).
	
	Ahora sí, mostramos el pseudocódigo del algoritmo (ver algoritmo \ref{algo:ILS}). Es importante notar que será el argumento $search\_algo$ el que determine si estamos ante la búsqueda local reiterada original (\textit{ILS}) o ante su variante hibridada (\textit{ILS-ES}). En concreto tendremos la \textit{ILS} cuando pasemos como argumento $search\_algo=local\_search\_from\_partition$ y la \textit{ILS-ES} cuando  pasemos $search\_algo=simulated\_annealing\_from\_partition$. Como ya se ha comentado anteriormente, estas dos funciones ($local\_search\_from\_partition$ y $simulated\_annealing\_from\_partition$) lanzan los algoritmos \textit{BL} y \textit{ES} con una solución que se pasa como argumento como punto de partida. Cabe mencionar que de nuevo se utiliza el argumento $restart\_mode$ para que ambas funciones devuelvan tanto la partición optimizada como el valor de la función objetivo asociado a esta. 
	
	Finalmente, respecto a los parámetros elegidos en nuestras ejecuciones, el número de búsquedas locales (o enfriamientos simulados) que fijamos es $n\_ls = 10$; y el número máximo de evaluaciones de la función objetivo en cada una es $evaluations\_per\_ls=10000$.
	
	
	\begin{algorithm}
	 	\caption{iterated\_local\_search}\label{algo:ILS}
	 	\KwData{Conjunto de datos $X$, lista de restricciones $const\_list$, número de clusters $k$, hiperparámetro $\lambda$, número de búsquedas locales (o enfriamientos simulados) a ejecutar $n\_ls$, número máximo de evaluaciones de la función objetivo en cada búsqueda local (o enfriamiento simulado) $evaluations\_per\_ls$, algoritmo de búsqueda a usar $search\_algo$ (\textit{BL} o \textit{ES})}
	 	\KwResult{Mejor solución encontrada $best\_partition$}
	 	\Begin{
	 	    n $\leftarrow$ longitud(X)\\
	 	    best\_partition, assignations\_counter $\leftarrow$ generate\_valid\_initial\_sol(X, k)\\
	 		best\_partition, best\_func\_value $\leftarrow$ search\_algo(best\_partition, assignations\_counter, X, const\_list, k, $\lambda$, max\_evaluations = evaluations\_per\_ls, restart\_mode = True)\\
	 		
            \ForEach{$i \in \{0,1,\dots, n\_ls-2\}$}{
                candidate\_partition $\leftarrow$ segment\_mutation\_operator(best\_partition, segment\_length = truncar(n * 0.1), k=k) \\
                assignations\_counter $\leftarrow$ cuenta\_asignaciones(candidate\_partition) \\
                \If{not asignaciones\_validas(assignations\_counter)}{
                    repair\_partition(candidate\_partition, assignations\_counter, k) \\
                }
                candidate\_partition, func\_value $\leftarrow$ search\_algo(candidate\_partition, assignations\_counter, X, const\_list, k, $\lambda$, max\_evaluations = evaluations\_per\_ls, restart\_mode = True)\\
                \If{func\_value $<$ best\_func\_value}{
                    -Criterio de aceptación el mejor. \\
                    best\_func\_value $\leftarrow$ func\_value \\
                    best\_partition $\leftarrow$ candidate\_partition \\
                }
            }
            return best\_partition \\
	 	}
	\end{algorithm}
	

	
	

    \clearpage
	\section{Procedimiento de desarrollo}
	
	Todo el código, desde la lectura de datos hasta los algoritmos, se ha implementado en $Python$ y se encuentra en la carpeta \textit{Software}. En concreto, los algoritmos se encuentran en el fichero \textit{algoritmos.py}. La función objetivo y estadísticos comunes a todos los algoritmos explicados en la sección \ref{sec:comun} se encuentran en el fichero \textit{funciones\_auxiliares\_y\_estadisticos.py}. Por otro lado, las funciones dedicadas a la lectura y carga de los conjuntos de datos se encuentran en el fichero \textit{leer\_datos.py}.
	
	Finalmente, se han desarrollado dos Jupyter Notebook (en $Python$ también). La primera con el objetivo de, usando las funciones definidas en los mencionados ficheros, hacer las ejecuciones que se nos requieren en el guión de una forma clara y sin distracciones y obtener las tablas que se nos pide (así como exportarlas al formato $Excel$). Esta Notebook se llama \textit{Ejecución\_y\_resultados.ipynb} y para la \textbf{replica de los resultados}, se recomienda ejecutarla directamente (las semillas están fijadas en ella). La segunda Notebook, \textit{Análisis\_resultados.ipynb}, se ha usado para analizar los resultados y realizar varias visualizaciones; se adjunta parte de ella en las siguientes páginas de esta memoria.
	
	\subsection{Entorno y paquetes}
Para el desarrollo del proyecto se ha trabajado con \href{https://www.anaconda.com/download/}{Anaconda} en Windows 10;  en concreto con Python y Jupyter Notebook. Un ordenador con una versión instalada de Python 3 será requerido. Específicamente, se ha usado la version Python 3.7.3. Para instalarla, puedes ejecutar en Anaconda Prompt:
\begin{lstlisting}
conda install python=3.7.3
\end{lstlisting}
Los paquetes NumPy (1.16.2) y Matplotlib (3.0.3) son usados, los puedes obtener con la herramienta pip:
\begin{lstlisting}
pip3 install numpy
pip3 install matplotlib
\end{lstlisting}

Los siguientes paquetes son también requeridos para ejecutar todo el código:
\begin{itemize}
    \item \href{https://seaborn.pydata.org/}{seaborn} (0.9.0) para mejorar las visualizaciones.
    \item \href{https://pandas.pydata.org/}{pandas} (0.24.2) para tratar con los resultados, crear las tablas y trabajar con ellas.
\end{itemize}
Los puedes instalar con conda:
\begin{lstlisting}
conda install -c anaconda seaborn
conda install -c anaconda pandas
\end{lstlisting}
	
\clearpage
\section{Análisis de los resultados}
En esta sección describiremos los experimentos realizados y estudiáremos los resultados obtenidos. 
\clearpage
17
\clearpage
18
\clearpage
19
\clearpage
20
\clearpage
\section{Extra:  Hibridación de ILS con la Búsqueda Local Suave (\textit{ILS-BLS})}

21

\clearpage
22
\clearpage
23


\end{document}